---
title: "CUNY SPS DATA-606 Final Project"
author: "Mike Silva"
date: "November 28, 2018"
output: 
  rmdformats::readthedown:
    highlight: kate
bibliography: Silva-Project.bib
---

## 1. Introduction

Finding affordable housing is a real challenge throughout the country.  The Rochester, New York metro is acclaimed fit its affordable housing. Recent data shows 81% of homes in the Rochester Metro are affordable [@nahb_hoi].  The median sales price in Rochester is $146,000, or half of the national average ($268,000).  With the relatively high family income, Rochester housing proves to be affordable.

There is considerable variety in the type of housing offered in Rochester. There are picturesque village homes built in the late 1800s. There are also suburban homes that are larger and newer. Can the square footage and age of a home estimate the sale price reliably?

## 2. Data

### Data Collection

The NYS Office of Real Property Services (NYS RPS) collects sales price data.  This administrative record data was scrapped using a [web scrapper](https://github.com/mikeasilva/CUNY-SPS/blob/21abb87c5a28204e75ba2944c574889e07289345/data/Build%20Monroe%20Real%20Property%20Data.py) in early November of 2018 [@mcrpp].  At that time, the data was as of 22 October 2018.

The NYS GIS clearinghouse compiles data on housing attributes [@nysgis]. This data originates from local assessors.  The 2017 data (latest available) was used.  The square footage of living space is the measurement of size in this study.  It is a quantitative measure.  

Data on the age of the home are transformed into eras (a categorical variable).  These eras come from an [article](https://www.forbes.com/sites/trulia/2013/05/02/american-homes-by-decade/#16c559821045) authored by Trulia's Chief Economist.  They are:

1. Before 1940s
2. Post-War Boom (1940 to 1979)
3. Last 30+ Years (1980+)

This study is purely observational in nature.  It does not assert these variables have a causal relationship.

### Data Preparation

The scrapped data is in a SQLite database stored on GitHub.  The first step is to download and extract this database locally.

```{r get_the_database, message=FALSE}
library(DBI)
library(dplyr)

# Download the database
if(!file.exists("Monroe Real Property Data.db")){
  if(!file.exists("Monroe Real Property Data.db.tar.gz")){
    # Download this specific commit from my GitHub repo
    download.file("https://github.com/mikeasilva/CUNY-SPS/raw/9a2023b57a466550646c9190076a53d51ef09ef2/data/Monroe%20Real%20Property%20Data.db.tar.gz", "Monroe Real Property Data.db.tar.gz")
  }
  # Decompress the file
  untar("Monroe Real Property Data.db.tar.gz")
  # Remove the file
  unlink("Monroe Real Property Data.db.tar.gz")
}
```

Now that the data is downloaded it is time to pull the sales price.  Each parcel is identified by a SWIS (Statewide Information System) and SBL (Section, Block, Lot) code.  Using this identifier, the next step is to find the latest sale date.

```{r read_db}
# Connect to the database
con <- dbConnect(RSQLite::SQLite(), "Monroe Real Property Data.db")

# Get the date of the latest sale for each property
results <- dbSendQuery(con, "SELECT MAX(sale_date) AS sale_date, SWIS, SBL FROM sales_data GROUP BY SWIS, SBL HAVING sale_date BETWEEN '2017-01-01' AND '2017-12-31'")
df <- dbFetch(results) 

dbClearResult(results)
```

Now the sale price can be pulled.  There should be `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` sale prices in the data set after the next step.

```{r sales_data}
df <- dbReadTable(con, "sales_data") %>% # Pull all the records
  distinct() %>% # remove any duplicates
  merge(df) %>% # Merge with the latest sale dates
  rename(Price = price)
```

The observations total `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)`.  It appears the merging process introduced some duplication into the data set.  This needs to be corrected.  Any parcel with more than one record will be dropped from the dataset.

```{r deduplication}
df <- df %>%
  select(SWIS, SBL) %>% # Select the unique identifiers
  group_by(SWIS, SBL) %>% 
  summarise(count = n()) %>% # Get the number of records
  ungroup() %>%
  filter(count == 1) %>% # Only keep the observations with one price
  select(-count) %>% # Drop the count variable
  merge(df) # Merge it with the original data frame to subset it
```

This leaves `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` unduplicated observations in the data set. Now that we have the outcome variable, it is time to add in the housing attributes.

```{r housing_attributes}
# Add in the housing attributes
df <- dbReadTable(con, "property_info") %>% # Pull all the records
  distinct() %>% # remove any duplicates
  merge(df) # Merge with the sale price

# Disconnect from the database
dbDisconnect(con)
```

There are `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations in the data set. This indicates the merging process did not introduce any duplication. Now that the housing attributes are added in, some odd observations need to be filtered out.

```{r filter_out_crap}
df <- df %>% # First filter out some obviously weird data
  filter(YR_BLT > 0) %>%
  filter(SQFT_LIV > 0) %>%
  filter(NBR_KITCHN > 0) %>%
  filter(NBR_BEDRM > 0)
```

After filtering there are `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations.  This is the final count of cases for the study.  The next step is to translate the year of construction into an era.

```{r era}
get_era <- function(year){
  if(year < 1940){
    return("1. Before 1940s")
  } else if(year < 1980){
    return("2. Post-War Boom")
  }else{
    return("3. Last 30+ Years")
  }
}

df <- df %>% 
  rowwise() %>%
  mutate(Era = get_era(YR_BLT)) %>%
  ungroup() %>%
  mutate(Era = as.factor(Era))
```

The last data preparation step is renaming and selecting the variables of interest.

```{r finalize_df}
df <- df %>%
  rename(Size = SQFT_LIV) %>%
  select(Price, Size, Era)
```

### Sanity Check

The American Community survey asks respondents about their home value. Assuming the sale price is comparable to the home value, how well does the study data represent the housing stock? The following figure takes the study data and compares with with Monroe County’s housing stock [@acs].

##### Figure 1. Percent of Homes by Price Groups and Data Set
```{r sanity_check, echo=FALSE}
library(ggplot2)
library(ggthemes)
library(tidyr)

get_acs_value <- function(price){
  if(price < 50000){
    return("1. Less than $50,000")
  } else if(price < 100000){
    return("2. $50,000 to $99,999")
  }else if(price < 150000){
    return("3. $100,000 to $149,999")
  }else if(price < 200000){
    return("4. $150,000 to $199,999")
  }else if(price < 300000){
    return("5. $200,000 to $299,999")
  }else if(price < 500000){
    return("6. $300,000 to $499,999")
  }else if(price < 1000000){
    return("7. $500,000 to $999,999")
  }else{
    return("8. $1,000,000 or more")
  }
}

check <- df %>%
  rowwise() %>%
  mutate(acs_value = get_acs_value(Price)) %>%
  ungroup() %>%
  group_by(acs_value) %>%
  summarise(study = n()) %>%
  arrange(acs_value)

# Monroe County Home Value Table C25075
check$acs <- c(9747, 32746, 54988, 38988, 34563, 14926, 3247, 924)

# Compute the shares
check <- check %>%
  mutate(acs_total = sum(acs)) %>%
  mutate(study_total = sum(study)) %>%
  mutate(acs_share = acs / acs_total) %>%
  mutate(study_share = study / study_total) %>%
  select(acs_value, acs_share, study_share) %>%
  rename(ACS = acs_share) %>%
  rename(Study = study_share) %>%
  gather(Dataset, Share, -acs_value) %>%
  rename(Value = acs_value)

ggplot(check, aes(x = reorder(Value, desc(Value)), Share, color=Dataset, fill=Dataset)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() + 
  scale_color_fivethirtyeight() +
  scale_fill_fivethirtyeight() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

The study data is generally fairly representative.  It under represent the two highest and two lowest categories. It also over represent home in the $100,000 to $149,999 price range. This could be due to different segments of the housing stock “moving” faster/more than other segments.

This data distribution depicted above raises some concern.  The data appear skewed.  The following density plot illustrates the skew.  The x-axis is stopped at half a million for readability, even though the largest sale price in the data set is `r round(max(df$Price) / 1000000, 1)` million dollars.

##### Figure 2: Sale Price Distribution
```{r price_hist, echo=FALSE, warning=FALSE}
ggplot(df, aes(Price)) +
  geom_density(fill = c("red"), color = c("red")) +
  scale_x_continuous(limits=c(0, 500000)) + 
  scale_fill_fivethirtyeight() +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

##### Table 1: Preliminary Price Summary
```{r price_table, echo=FALSE}
library(kableExtra)

summary(df$Price) %>%
  as.array() %>%
  as.data.frame() %>%
  rename(Metric = Var1) %>%
  rename(Value = Freq) %>%
  kable() %>%
  kable_styling()
```

This confirms there are extreme housing prices in the data set.  The data is also not normal.  To correct this, the price will be put on a log scale and outliers will be removed.  

```{r log_transform}
# Log transformation
df <- df %>%
  mutate(Log_Price = log(Price))

# Remove the outliers
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

valid_prices <- unique(remove_outliers(df$Log_Price))
min_price <- min(valid_prices, na.rm=TRUE)
max_price <- max(valid_prices, na.rm=TRUE)

df <- df %>%
  filter(Log_Price > min_price) %>%
  filter(Log_Price < max_price)
```

Now that the outliers have been removed we have `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations in our data set.  Here is what the log of the prices looks like:

##### Figure 3. Log Price Distribution
```{r log_price_hist, echo=FALSE, warning=FALSE}
ggplot(df, aes(Log_Price)) +
  geom_histogram(bins=30, fill = c("red"), color = c("red")) +
  scale_fill_fivethirtyeight() +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

### Study Data

As previously stated there are `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations in this data set.  Each case represents the most recent sale price for a residential property occuring in 2017, ranging in price from `r paste0("$", prettyNum(min(df$Price), big.mark = ",", scientific = FALSE))` to  `r paste0("$", prettyNum(max(df$Price), big.mark = ",", scientific = FALSE))`.  On a log scale these prices range from `r paste0("$", prettyNum(min(df$Log_Price), big.mark = ",", scientific = FALSE))` to  `r paste0("$", prettyNum(max(df$Log_Price), big.mark = ",", scientific = FALSE))`. 

## 3. Exploratory Data Analysis

As mentioned in the introduction, the median sale price for a home in the Rochester Metro is $146,000 [@nahb_hoi].  The median sale price in the study dataset is `r paste0("$", prettyNum(median(df$Price), big.mark = ",", scientific = FALSE))` which is comparable.

This study posits the size and era a home was built are reliable predictors of the price (on a log scale).  The following scatter plot illustrates the relationship observed in the data. 

##### Figure 4: Log Sales Price and Size by Era
```{r fig.height=10, echo=FALSE}
ggplot(df, aes(Log_Price, Size, color = Era)) + 
  geom_point() + 
  scale_color_fivethirtyeight() +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  facet_grid(Era ~ .)
```

It is clear that price and size are related.  The relationship between price and era is not as clear  The following visualization and summary table examines this relationship further:

##### Figure 5: Log Sales Price by Era
```{r era_by_price_boxplot, echo=FALSE}
ggplot(df, aes(x=Era, y=Log_Price, color = Era)) + 
  geom_boxplot() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  theme(legend.position = "None")
```

##### Table 2: Log Sales Price by Era Summary
```{r price_by_era_table, echo=FALSE}
df %>%
  group_by(Era) %>%
  summarise(min = min(Log_Price), mean = mean(Log_Price), median = median(Log_Price), max = max(Log_Price)) %>%
  kable() %>%
  kable_styling()
```

The older houses tend to fetch less in the market than the newer houses.  There also does not seem to be much difference between the pre and post-war housing.

### Checking for Multicollinearity

The two explanatory variables might be related.  It is reasonable to believe that homes increased in size over time.  The plot below will display how the size and era are related.

##### Figure 6: Size by Era
```{r size_by_era_boxplot, echo=FALSE}
ggplot(df, aes(x=Era, y=Size, color = Era)) + 
  geom_boxplot() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  theme(legend.position = "None")
```

##### Table 3: Size by Era Summary
```{r size_by_era_table, echo=FALSE}
df %>%
  group_by(Era) %>%
  summarise(min = min(Size), mean = mean(Size), median = median(Size), max = max(Size)) %>%
  kable() %>%
  kable_styling()
```

The newer houses are generally larger than the older houses.  This trend does not seem as pronounced as I expected it would.

## 4. Inference

In order to evaluate how well the model generalizes the data will be split into training/test sets using an 80/20 split.

```{r train_test_split, message=FALSE}
library(caret)
set.seed(12345)
in_training <- createDataPartition(df$Log_Price, p = 0.8, list = FALSE)
training <- df[ in_training, ]
testing  <- df[-in_training, ]
```

There are `r prettyNum(nrow(training), big.mark = ",", scientific = FALSE)` observations in the training data set.  With the data split, the OLS line can be fit to the data.

```{r ols}
fit <- lm(Log_Price ~ Size + Era , training)
```

This is the summary of the model:

```{r summarize_model, comment=NA}
summary(fit)
```

This model's coefficients are a little hard to interpret given that they are on a log scale.  Yet one observes that all the coefficients are statistically significant.  The square footage of living space (size) is positively correlated with the home price.  Also the later the home is built is positively correlated with the price.  Only 60% of the variability in the log of the sales price is explained by these two variables.

```{r, echo=FALSE, eval=FALSE}
exp(coef(fit))
```

### Residual Examination

##### Figure 7: Residuals Scatterplot
```{r residual_plot, echo=FALSE}
ggplot(fit, aes(x = c(1:length(resid(fit))), y = resid(fit))) + 
  geom_point(color = "red") + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans")
```

The above figure is the residuals ordered by their index.  There is no clear pattern in this figure.

##### Figure 7: Residuals Q to Q
```{r qqplot_residuals,echo=FALSE}
ggplot(fit, aes(sample=resid(fit)))+
  stat_qq(color="red") +
  geom_qq_line() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  theme(legend.position = "None")
  
```

The Q to Q plot above indicates that the residuals are more linear near the middle of the distribution.  But there are some major problems in the tails.

### Generalization

In order to assess how well this model generalizes, predictions for the testing data set will be made and compared to the actual.

```{r test_predictions}
predictions <- predict(fit, testing, interval = "confidence")
testing$Log_Price_hat <- predictions[, 1]
testing$Price_hat <- exp(testing$Log_Price_hat)
testing$Log_Price_hat_lower <- predictions[, 2]
testing$Price_hat_lower <- exp(testing$Log_Price_hat_lower)
testing$Log_Price_hat_upper <- predictions[, 3]
testing$Price_hat_upper <- exp(testing$Log_Price_hat_upper)
```

Let's visualize the predictions vs the actual sales prices.  In the following plot the sales price is on the x-axis and the prediction is on the y-axis.  Values are per one thousand.  The black dashed line represents where the prediction and the actual values are the same.  The blue are under estimates, red are over estimates, and the green points have a price that falls within the prediction's confidence interval.

##### Figure 8: Test Set Actual vs Model Prediction (in thousands)
```{r prediction_plot, echo=FALSE, message=FALSE, warning=FALSE}
over_under_equal <- function(price, lower, upper){
  if(price > upper){"Over Estimate"}
  else if(price < lower){"Under Estimate"}
  else {"Within CI"}
}
testing <- testing %>%
  rowwise() %>%
  mutate(Accuracy = over_under_equal(Price, Price_hat_lower, Price_hat_upper)) %>%
  ungroup

ggplot(testing, aes(Price_hat / 1000, Price / 1000, color = Accuracy)) + 
  geom_point() + 
  scale_color_fivethirtyeight() +
  xlim(0, max(testing$Price) / 1000) +
  ylim(0, max(testing$Price) / 1000) +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = 2) +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

There are `r nrow(testing[testing$Accuracy=="Over Estimate", ])` cases where the model over estimates the home price, and `r nrow(testing[testing$Accuracy=="Under Estimate", ])` where it under estimates the home price. `r nrow(testing[testing$Accuracy=="Within CI", ])` are in within the confidence interval.  Let's examine to what degree it over/under estimates.  The model does not seem to accurately depict reality.

```{r test_error}
testing <- testing %>%
  mutate(Error = Price_hat - Price)
```

##### Figure 9: Prediction Error Distribution
```{r prediction_error_distribution, echo=FALSE}
testing %>%
  ggplot(aes(Error / 1000)) +
  geom_histogram(bins = 50, fill="Red") + 
  scale_fill_fivethirtyeight() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  guides(fill = FALSE, color = FALSE)
```

##### Table 4: Prediction Error Summary
```{r error_table, echo=FALSE}
summary(testing$Error) %>%
  as.array() %>%
  as.data.frame() %>%
  rename(Metric = Var1) %>%
  rename(Value = Freq) %>%
  kable() %>%
  kable_styling()
```

There is a wide variety in the errors.  Let's look at the distribution of the error in absolute terms expressed as a share of the total price.  It would be nice to see errors in the 5% range

```{r error_share}
testing <- testing %>%
  mutate(Error_Share = (abs(Error) / Price) * 100)
```

##### Figure 10. Cumulative Distribution of Error Share (5% cutoff in red)
```{r error_share_plot, echo=FALSE}
testing %>%
  ggplot(aes(Error_Share)) + 
  stat_ecdf() + 
  geom_vline(xintercept = 5, color = "Red", linetype = "dashed") +
  scale_fill_fivethirtyeight() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  guides(fill = FALSE, color = FALSE)
```

Only `r round((nrow(testing[testing$Error_Share < 5, ]) / nrow(testing)) * 100, 0)`% of the testing set observations have an error which is +/-5% of the estimate.  Once again the model seems to be a poor predictor.

## 5. Conclusion

This study set out to see if the size of a home and the era in which it was built could be used to get an estimate of the sale price. While these variables seem to be relevant, predictions made only using the variables are largely inaccurate.  It appears that the relationship is more complicated. Future research could expand the model.  A location variable would be a next step along with the size of the plot.  A different modeling approach should be employed as the relationship appears to be non-linear.

## 6. References