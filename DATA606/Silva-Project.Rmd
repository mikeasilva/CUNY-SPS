---
title: "CUNY SPS DATA-606 Final Project"
author: "Mike Silva"
date: "November 28, 2018"
output: 
  rmdformats::readthedown:
    highlight: kate
bibliography: Silva-Project.bib
---

## 1. Introduction

Finding affordable housing is a real challenge throughout the country.  The Rochester, New York metro is acclaimed as a place with affordable housing. Recent data show 81% of the housing in the Rochester Metro is affordable [@nahb_hoi].  The median sales price for a home in Rochester is $146,000.  This is about half as much as the national average ($268,000).  With the high family income, housing proves to be affordable.

There is considerable variety in the type of housing offered in Rochester. There are picturesque village homes built in the late 1800s.  There are also suburban homes that are larger and newer. Can the square footage and age of a home reliabily estimate the sale price?

## 2. Data

### Data Collection

The NYS Office of Real Property Services (NYS RPS) collects sales price data from adminstrative records.  The data for this study were scrapped using a [web scrapper](https://github.com/mikeasilva/CUNY-SPS/blob/21abb87c5a28204e75ba2944c574889e07289345/data/Build%20Monroe%20Real%20Property%20Data.py) in early November of 2018 [@mcrpp].  At that time the data was synced with NYS RPS on 22 October 2018.

The NYS GIS clearinghouse compiled data on housing attributes [@nysgis]. The 2017 data is used in this study.  The square footage of living space is the measurement of size in this study.  It is a quantitative measure.  

Data on the age of the home are transformed into eras (a categorical variable).  These eras come from an [article](https://www.forbes.com/sites/trulia/2013/05/02/american-homes-by-decade/#16c559821045) from the Trulia's Chief Economist.  They are:

1. Before 1940s
2. Post-War Boom (1940 to 1979)
3. Last 30+ Years (1980+)

This study is purely observational in nature.  It does not assert that these variables have a causal relationship nor should it be interpreted that way.

Scope of inference - generalizability: Identify the population of interest, and whether the findings from this analysis can be generalized to that population, or, if not, a subsection of that population. Explain why or why not. Also discuss any potential sources of bias that might prevent generalizability.

### Data Preparation

The scrapped data is in a SQLite database stored in GitHub.  The first step is to download and extract this database.

```{r get_the_database, message=FALSE}
library(DBI)
library(dplyr)

# Download the database
if(!file.exists("Monroe Real Property Data.db")){
  if(!file.exists("Monroe Real Property Data.db.tar.gz")){
    # Download this specific commit from my GitHub repo
    download.file("https://github.com/mikeasilva/CUNY-SPS/raw/9a2023b57a466550646c9190076a53d51ef09ef2/data/Monroe%20Real%20Property%20Data.db.tar.gz", "Monroe Real Property Data.db.tar.gz")
  }
  # Decompress the file
  untar("Monroe Real Property Data.db.tar.gz")
  # Remove the file
  unlink("Monroe Real Property Data.db.tar.gz")
}
```

Now that the data is downloaded it is time to pull the sales price.  Each parcel is identified by a SWIS (Statewide Information System) and SBL (Section, Block, Lot) code.  Using this identifier, the next step is to find the latest sale date.  

```{r read_db}
# Connect to the database
con <- dbConnect(RSQLite::SQLite(), "Monroe Real Property Data.db")

# Get the date of the latest sale for each property
results <- dbSendQuery(con, "SELECT MAX(sale_date) AS sale_date, SWIS, SBL FROM sales_data GROUP BY SWIS, SBL HAVING sale_date BETWEEN '2017-01-01' AND '2017-12-31'")
df <- dbFetch(results) 

dbClearResult(results)
```

Now the sale price can be pulled.  There should be `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` sale prices in the data set after the next step.

```{r sales_data}
df <- dbReadTable(con, "sales_data") %>% # Pull all the records
  distinct() %>% # remove any duplicates
  merge(df) %>% # Merge with the latest sale dates
  rename(Price = price)
```

The observations total `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)`.  It appears the merging process introduced some duplication into the data set.  This needs to be corrected.  Any parcel with more than one record will be dropped from the dataset.

```{r deduplication}
df <- df %>%
  select(SWIS, SBL) %>% # Select the unique identifiers
  group_by(SWIS, SBL) %>% 
  summarise(count = n()) %>% # Get the number of records
  ungroup() %>%
  filter(count == 1) %>% # Only keep the observations with one price
  select(-count) %>% # Drop the count variable
  merge(df) # Merge it with the original data frame to subset it
```

This leaves `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` unduplicated observations in the data set. Now that we have the outcome variable, it is time to add in the housing attributes.

```{r housing_attributes}
# Add in the housing attributes
df <- dbReadTable(con, "property_info") %>% # Pull all the records
  distinct() %>% # remove any duplicates
  merge(df) # Merge with the sale price

# Disconnect from the database
dbDisconnect(con)
```

There are `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations in the data set. This indicates the merging process did not introduce any dupplication. Now that the housing attributes are added in, some odd observations need to be filtered out.

```{r filter_out_crap}
df <- df %>% # First filter out some obviously weird data
  filter(YR_BLT > 0) %>%
  filter(SQFT_LIV > 0) %>%
  filter(NBR_KITCHN > 0) %>%
  filter(NBR_BEDRM > 0)
```

After filtering there are `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations.  This is the final count of cases for the study.  The last data preperation step is to change the year built quantitative variable into a categorical variable and only keep the variables of interest.

```{r finalize_df}
get_era <- function(year){
  if(year < 1940){
    return("1. Before 1940s")
  } else if(year < 1980){
    return("2. Post-War Boom")
  }else{
    return("3. Last 30+ Years")
  }
}

df <- df %>% 
  rowwise() %>%
  mutate(Era = get_era(YR_BLT)) %>%
  ungroup() %>%
  mutate(Era = as.factor(Era)) %>%
  rename(Size = SQFT_LIV) %>%
  select(Price, Size, Era)
```

### Sanity Check

The American Community survey asks respondents about their home value.  If we assume that the sale price is equivalent to the home value, how well does the study data represent the housing stock?  The following figure takes the study data and compares with with Monroe County's housing stock [@acs].

##### Figure 1. Percent of Homes by Price Groups and Data Set
```{r sanity_check, echo=FALSE}
library(ggplot2)
library(ggthemes)
library(tidyr)

get_acs_value <- function(price){
  if(price < 50000){
    return("1. Less than $50,000")
  } else if(price < 100000){
    return("2. $50,000 to $99,999")
  }else if(price < 150000){
    return("3. $100,000 to $149,999")
  }else if(price < 200000){
    return("4. $150,000 to $199,999")
  }else if(price < 300000){
    return("5. $200,000 to $299,999")
  }else if(price < 500000){
    return("6. $300,000 to $499,999")
  }else if(price < 1000000){
    return("7. $500,000 to $999,999")
  }else{
    return("8. $1,000,000 or more")
  }
}

check <- df %>%
  rowwise() %>%
  mutate(acs_value = get_acs_value(Price)) %>%
  ungroup() %>%
  group_by(acs_value) %>%
  summarise(study = n()) %>%
  arrange(acs_value)

# Monroe County Home Value Table C25075
check$acs <- c(9747, 32746, 54988, 38988, 34563, 14926, 3247, 924)

# Compute the shares
check <- check %>%
  mutate(acs_total = sum(acs)) %>%
  mutate(study_total = sum(study)) %>%
  mutate(acs_share = acs / acs_total) %>%
  mutate(study_share = study / study_total) %>%
  select(acs_value, acs_share, study_share) %>%
  rename(ACS = acs_share) %>%
  rename(Study = study_share) %>%
  gather(Dataset, Share, -acs_value) %>%
  rename(Value = acs_value)

ggplot(check, aes(x = reorder(Value, desc(Value)), Share, color=Dataset, fill=Dataset)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() + 
  scale_color_fivethirtyeight() +
  scale_fill_fivethirtyeight() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

The study data under represents the two highest and two lowest categories.  It also overrepresents the $100,000 to $149,999 price range.  This could indicate that different segments of the housing stock "moved" faster/more than other segements.

This data above raise some concern.  The data appears to be skewed.  The following density plot illustrates the skew.  The x-axis is stopped at half a million even though the largest sale price in the data set is `r round(max(df$Price) / 1000000, 1)` million dollars.

##### Figure 2: Sale Price Distribution
```{r price_hist, echo=FALSE, warning=FALSE}
ggplot(df, aes(Price)) +
  geom_density(fill = c("red"), color = c("red")) +
  scale_x_continuous(limits=c(0, 500000)) + 
  scale_fill_fivethirtyeight() +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

##### Table 1: Preliminary Price Summary
```{r price_table, echo=FALSE}
library(kableExtra)

summary(df$Price) %>%
  as.array() %>%
  as.data.frame() %>%
  rename(Metric = Var1) %>%
  rename(Value = Freq) %>%
  kable() %>%
  kable_styling()
```

There are extreme housing prices in the data set.  The data is also not normal.  In order to correct this the price will be on a log scale and the outliers will be removed.  

```{r log_transform}
# Log transformation
df <- df %>%
  mutate(Log_Price = log(Price))

# Remove the outliers
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

valid_prices <- unique(remove_outliers(df$Log_Price))
min_price <- min(valid_prices, na.rm=TRUE)
max_price <- max(valid_prices, na.rm=TRUE)

df <- df %>%
  filter(Log_Price > min_price) %>%
  filter(Log_Price < max_price)
```

Now that the outliers have been removed we have `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations in our data set.  Here is what the log of the prices looks like:

##### Figure 3. Log Price Distribution
```{r log_price_hist, echo=FALSE, warning=FALSE}
ggplot(df, aes(Log_Price)) +
  geom_histogram(bins=30, fill = c("red"), color = c("red")) +
  scale_fill_fivethirtyeight() +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") 
```

### Study Data

As previously stated there are `r prettyNum(nrow(df), big.mark = ",", scientific = FALSE)` observations in this data set.  Each case represents the most recent sale price for a residential property occuring in 2017, ranging in price from `r paste0("$", prettyNum(min(df$Price), big.mark = ",", scientific = FALSE))` to  `r paste0("$", prettyNum(max(df$Price), big.mark = ",", scientific = FALSE))`.  On a log scale these prices are ranging in price from `r paste0("$", prettyNum(min(df$Log_Price), big.mark = ",", scientific = FALSE))` to  `r paste0("$", prettyNum(max(df$Log_Price), big.mark = ",", scientific = FALSE))`. 

## 3. Exploratory Data Analysis

As mentioned in the introduction, the median sale price for a home in the Rochester Metro is $146,000 [@nahb_hoi].  The median sale price in the study dataset is `r paste0("$", prettyNum(median(df$Price), big.mark = ",", scientific = FALSE))` which is pretty comparable.

This study posits the size and era a home was built are reliable predictors of the price (on a log scale).  The following scatterplot illustrates the relationship observed in the data. 

##### Figure 4: Log Sales Price and Size by Era
```{r fig.height=10, echo=FALSE}
ggplot(df, aes(Log_Price, Size, color = Era)) + 
  geom_point() + 
  scale_color_fivethirtyeight() +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  facet_grid(Era ~ .)
```

It is clear to see that price and size are related.  The relationship between price and era is not as apparent.  The following visualization and summary table examines this relationship:

##### Figure 5: Log Sales Price by Era
```{r era_by_price_boxplot, echo=FALSE}
ggplot(df, aes(x=Era, y=Log_Price, color = Era)) + 
  geom_boxplot() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  theme(legend.position = "None")
```

##### Table 2: Log Sales Price by Era Summary
```{r price_by_era_table, echo=FALSE}
df %>%
  group_by(Era) %>%
  summarise(min = min(Log_Price), mean = mean(Log_Price), median = median(Log_Price), max = max(Log_Price)) %>%
  kable() %>%
  kable_styling()
```

The older houses tend to fetch less in the market than the newer houses.  There doesn't seem to be much difference between the pre and post-war housing.

### Checking for Multicollinearity

The two explanatory variables might be related.  It is reasonable to believe that home increased in size over time.  The plot below will check to see if the size and era are related.

##### Figure 6: Size by Era
```{r size_by_era_boxplot, echo=FALSE}
ggplot(df, aes(x=Era, y=Size, color = Era)) + 
  geom_boxplot() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  theme(legend.position = "None")
```

##### Table 3: Size by Era Summary
```{r size_by_era_table, echo=FALSE}
df %>%
  group_by(Era) %>%
  summarise(min = min(Size), mean = mean(Size), median = median(Size), max = max(Size)) %>%
  kable() %>%
  kable_styling()
```

The newer houses are generally larger than the older houses.  This trend does not seem as pronounced as I expected it would.

## 4. Inference

In order to evaluate how well the model generalizes the data will be split into training/test sets using an 80/20 split.

```{r train_test_split, message=FALSE}
library(caret)
set.seed(12345)
in_training <- createDataPartition(df$Log_Price, p = 0.8, list = FALSE)
training <- df[ in_training, ]
testing  <- df[-in_training, ]
```

There are `r prettyNum(nrow(training), big.mark = ",", scientific = FALSE)` observations in the training data set.  With the data split, the OLS line can be fit to the data.

```{r ols}
fit <- lm(Log_Price ~ Size + Era , training)
```

This is the summary of the model:

```{r summarize_model, comment=NA}
summary(fit)
```

This model's coefficients are a little hard to interpret given that they are on a log scale.  However one observes that all of the coefficients are statistically significant.  The square footage of living space (size) is positively correlated with the home price.  Also the later the home is built is positively correlated with the price.  60% of the variability in the log of the sales price is explained by these two variables.

```{r, echo=FALSE, eval=FALSE}
exp(coef(fit))
```

### Model Examination



##### Figure X: Residuals Scatterplot
```{r residual_plot, echo=FALSE}
ggplot(fit, aes(x = c(1:length(resid(fit))), y = resid(fit))) + 
  geom_point(color = "red") + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans")
```

ADD TEXT ABOUT THE PLOT

```{r qqplot_residuals}
ggplot(fit, aes(sample=resid(fit)))+
  stat_qq(color="red") +
  geom_qq_line() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  theme(legend.position = "None")
  
```

The Q to Q plot above indicates that the residuals are more linear near the middle of the distribution but there are some major problems in the tails.

### Generalization

In order to assess how well this model generalizes, predictions for the testing data set will be made and compared to the actual.

```{r test_predictions}
testing$Log_Price_hat <- predict(fit, testing)
testing$Price_hat <- exp(testing$Log_Price_hat)
```

Let's visualize the predictions vs the actual sales prices.  In the following plot the sales price is on the x-axis and the prediction is on the y-axis.  Values are per one thousand.  The black dashed line represents where the prediction and the actual values are the same.

##### Figure X: Test Set Actual vs Model Prediction (in thousands)
```{r prediction_plot, echo=FALSE, message=FALSE, warning=FALSE}
over_under_equal <- function(price, price_hat){
  if(price == price_hat){"Equal"}
  else if(price > price_hat){"Under"}
  else {"Over"}
}
testing <- testing %>%
  rowwise() %>%
  mutate(Accuracy = over_under_equal(Price, Price_hat)) %>%
  ungroup

ggplot(testing, aes(Price / 1000, Price_hat / 1000, color = Accuracy)) + 
  geom_point() + 
  scale_color_fivethirtyeight() +
  xlim(0, max(testing$Price) / 1000) +
  ylim(0, max(testing$Price) / 1000) +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = 2) +
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  guides(fill = FALSE, color = FALSE)
```

There are `r nrow(testing[testing$Accuracy=="Over", ])` cases where the model over estimates and `r nrow(testing[testing$Accuracy=="Under", ])` where it under estimates the home price. Let's examine to what degree it over/under estimates.

```{r test_error}
testing <- testing %>%
  mutate(Error = Price_hat - Price)
```

##### Figure X: Prediction Error Distribution
```{r prediction_error_distribution, echo=FALSE}
testing %>%
  ggplot(aes(Error / 1000)) +
  geom_histogram(bins = 50, fill="Red") + 
  scale_fill_fivethirtyeight() + 
  theme_fivethirtyeight(base_size = 12, base_family = "sans") +
  guides(fill = FALSE, color = FALSE)
```

##### Table 4?: Prediction Error Summary
```{r error_table, echo=FALSE}
summary(testing$Error) %>%
  as.array() %>%
  as.data.frame() %>%
  rename(Metric = Var1) %>%
  rename(Value = Freq) %>%
  kable() %>%
  kable_styling()
```

There is a wide variety in the errors.  It appears that the model tends to be really good or really bad.

```{r echo=F, eval=F}
If your data fails some conditions and you canâ€™t use a theoretical method, then you should use simulation. If you can use both methods, then you should use both methods. It is your responsibility to figure out the appropriate methodology.



Check conditions

Theoretical inference (if possible) - hypothesis test and confidence interval

Simulation based inference - hypothesis test and confidence interval

Brief description of methodology that reflects your conceptual understanding
```

## 5. Conclusion

This study set out to see if the size of a home and the era in which it was built could be used to get an estimate of the sale price.  While these variables do seem to be relavant in predicting the home value, it appears that it is more complicated than just that.  Future research could include extending the model to capture a location variable, or employ a different modeling approach as  it appears that the relationship may be non-linear.

## 6. References