---
title: "DATA 624 Homework 5"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "show"
---


```{r setup, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(fpp2)
library(kableExtra)
library(gridExtra)
library(readxl)
options(scipen=999)
```

## Question 7.1

Consider the `pigs` series — the number of pigs slaughtered in Victoria each month.

a. Use the ses() function in R to find the optimal values of $\alpha$ and $\iota_0$, and generate forecasts for the next four months.

```{r}
ses_pigs <- ses(pigs, h = 4)
summary(ses_pigs)
```

**The `ses` function generates and $\alpha$ of `r ses_pigs$model$par[1]` and a $\iota_0$ of `r ses_pigs$model$par[2]`.**

b. Compute a 95% prediction interval for the first forecast using  $\hat{y} \pm$ 1.96 $s$ where $s$ is the standard deviation of the residuals. 

```{r}
# Get the first forecast
y_hat <- ses_pigs$mean[1]
# Get the standard deviation of the residuals
s <- sd(ses_pigs$residuals)
lower_ci <- y_hat - 1.96 * s
upper_ci <- y_hat + 1.96 * s
```

**The 95% CI is from `r round(lower_ci, 0)` to `r round(upper_ci, 0)`,**

c. Compare your interval with the interval produced by R.

**The 95% CI produced by R is `r round(78611.97, 0)` to `r round(119020.8, 0 )`.R's CI is slightly wider than the CI we computed.**

## Question 7.5

Data set `books` contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days’ sales for paperback and hardcover books.

a. Plot the series and discuss the main features of the data.

```{r}
autoplot(books) + 
  ggtitle("Daily Sale of Books")
```

**The series has an upwards trend.  There is only 30 days of data so I can't really speak to an seasonality or weekly effects with much confidence.  The paperback sales lagging hardback, or hardback and paperback being counter cyclical, but I would stress it's really too soon to make these claims.**

b. Use the `ses()` function to forecast each series, and plot the forecasts.

```{r}
ses_paperback <- ses(books[, 1], h = 4)
autoplot(ses_paperback) +
  ylab("Paperback Book Sales")
```

```{r}
ses_hardback <- ses(books[, 2], h = 4)
autoplot(ses_hardback) +
  ylab("Hardback Book Sales")
```

c. Compute the RMSE values for the training data in each case.

```{r}
accuracy(ses_paperback)
```

```{r}
accuracy(ses_hardback)
```

**The RMSE for the paperback model is `r round(accuracy(ses_paperback)[2], 2)` and `r round(accuracy(ses_hardback)[2], 2)` for the hardback model.**

## Question 7.6

We will continue with the daily sales of paperback and hardcover books in data set `books`.

a. Apply Holt’s linear method to the `paperback` and `hardback` series and compute four-day forecasts in each case.

```{r}
holt_paperback <- holt(books[, 1], h = 4)
summary(holt_paperback)
```

```{r}
holt_hardback <- holt(books[, 2], h = 4)
autoplot(holt_hardback) +
  ylab("Hardback Book Sales")
summary(holt_hardback)
```

b. Compare the RMSE measures of Holt’s method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt’s method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.

**Holt's method does a better job than the SES models, because the RMSE is smaller for both the paperback and hardback series.  This is understandable because Holt's includes a trend component.  SES assumes there is no trend.  This does not really fit these timeseries.**

```{r, echo=FALSE}
data.frame(Type = c('Paperback', 'Hardback'),
           SES = c(accuracy(ses_paperback)[2], accuracy(ses_hardback)[2]),
           Holt = c(accuracy(holt_paperback)[2], accuracy(holt_hardback)[2])
           ) %>%
  rename(`Holt's Method` = Holt) %>%
  kable() %>%
  kable_styling()
```

c. Compare the forecasts for the two series using both methods. Which do you think is best?

```{r}
p1 <- autoplot(ses_paperback) + ylab("Paperback Book Sales") + ggtitle("SES Forecast")
p2 <- autoplot(holt_paperback) + ylab("Paperback Book Sales") + ggtitle("Holt's Forecast")
grid.arrange(p1, p2, ncol = 2)
```

```{r}
p1 <- autoplot(ses_hardback) + ylab("Hardback Book Sales") + ggtitle("SES Forecast")
p2 <- autoplot(holt_hardback) + ylab("Hardback Book Sales") + ggtitle("Holt's Forecast")
grid.arrange(p1, p2, ncol = 2)
```

**I think Holt's method is a better forecast.  I like that factors in the trend.**

d. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using `ses` and `holt`.

```{r}
cat("Paperback:", holt_paperback$mean[1] - 1.96 * accuracy(holt_paperback)[2], "to", holt_paperback$mean[1] + 1.96 * accuracy(holt_paperback)[2])
```
```{r}
cat("Hardback:", holt_hardback$mean[1] - 1.96 * accuracy(holt_hardback)[2], "to", holt_hardback$mean[1] + 1.96 * accuracy(holt_hardback)[2])
```

**The 95% CI for the paperback series generated by Holt's method is 143.9130  to 275.0205.  This is wider than the CI computed above.  The CI for hardbacks from Holt's method is 192.9222 to 307.4256, which again is wider than the CI above.**


## Question 7.7

For this exercise use data set `eggs`, the price of a dozen eggs in the United States from 1900–1993. Experiment with the various options in the `holt()` function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.

[Hint: use `h=100` when calling `holt()` so you can clearly see the differences between the various options when plotting the forecasts.]

Which model gives the best RMSE?

```{r}
h <- 100

holt_eggs <- holt(eggs, h=h)
box_cox_eggs <- holt(eggs, h=h, lambda=TRUE)
damped_eggs <- holt(eggs, h=h, damped=TRUE)
damped_box_cox_eggs <- holt(eggs, h=h, damped=TRUE, lambda=TRUE)
exponential_eggs <- holt(eggs, h=h, exponential=TRUE)

autoplot(holt_eggs) + ggtitle("Holt's Method") 
autoplot(damped_eggs) + ggtitle("Damped")
autoplot(box_cox_eggs) + ggtitle("Box-Cox")
autoplot(damped_box_cox_eggs) + ggtitle("Damped & Box-Cox")
autoplot(exponential_eggs) + ggtitle("Exponential")
```



```{r}
get_rmse <- function(model){
  accuracy(model)[2]
}

Model <- c("Holt's Linear", 
           "Box-Cox Transformed",
           "Damped",
           "Damped and Box-Cox",
           "Exponential")

RMSE <- c(get_rmse(holt_eggs), 
          get_rmse(box_cox_eggs),
          get_rmse(damped_eggs),
          get_rmse(damped_box_cox_eggs),
          get_rmse(exponential_eggs))

eggs_rmse_df <- data.frame(Model, RMSE) 

eggs_rmse_df %>%
  kable() %>%
  kable_styling()
```

**The model with the lowest (best) RMSE is the `r eggs_rmse_df[eggs_rmse_df$RMSE == min(eggs_rmse_df$RMSE),]$Model` model.**


## Question 7.8

Recall your retail time series data (from Exercise 3 in Section 2.10).

a. Why is multiplicative seasonality necessary for this series?

**As we have seen in the previous sections, the variability in the series is increasing over time.  This calls for a method that accounts for multiplicative seasonality.  Here's the plot in case you have forgotten what the time series looks like:**

```{r}
retaildata <- read_excel("retail.xlsx", skip = 1)
retail_ts <- ts(retaildata[, "A3349873A"], frequency = 12, start = c(1982, 4))
autoplot(retail_ts) + ylab("Retail Sales")
```

b. Apply Holt-Winters’ multiplicative method to the data. Experiment with making the trend damped.

```{r}
hw_multiplicative <- hw(retail_ts, seasonal = "multiplicative")
autoplot(hw_multiplicative) + 
  ggtitle("Multiplicative") + 
  ylab("Retail Sales")
summary(hw_multiplicative)

hw_multiplicative_damped <- hw(retail_ts, seasonal = "multiplicative", damped = TRUE)
autoplot(hw_multiplicative_damped) + 
  ggtitle("Multiplicative & Damped") + 
  ylab("Retail Sales")
summary(hw_multiplicative_damped)
```

c. Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?

```{r}
cat("RMSE of Multiplicative = ", accuracy(hw_multiplicative)[2])
```

```{r}
cat("RMSE of Multiplicative & Damped = ", accuracy(hw_multiplicative_damped)[2])
```

**The non-damped model is preforming better.**

d. Check that the residuals from the best method look like white noise.

```{r}
checkresiduals(hw_multiplicative)
```

**The residuals looks like white noise to me.  They are normally distributed with a mean of zero, with no pattern over time.**

e. Now find the test set RMSE, while training the model to the end of 2010. Can you beat the seasonal naïve approach from Exercise 8 in Section 3.7?

```{r}
retail_train <- window(retail_ts, end = c(2010, 12))
retail_test <- window(retail_ts, start = 2011)

get_retail_test_rmse <- function(model){
  accuracy(model, retail_test)[4]
}

Model <- c("Seasonal Naïve (Baseline)", 
           "SES",
           "Holt's Method",
           "Damped Holt's Method",
           "Holt-Winters Additive",
           "Holt-Winters Multiplicative",
           "Damped Holt-Winters Additive",
           "Damped Holt-Winters Multiplicative")

RMSE <- c(get_retail_test_rmse(snaive(retail_train)), 
          get_retail_test_rmse(ses(retail_train)),
          get_retail_test_rmse(holt(retail_train)),
          get_retail_test_rmse(holt(retail_train, damped = TRUE)),
          get_retail_test_rmse(hw(retail_train, seasonal = "additive")),
          get_retail_test_rmse(hw(retail_train, seasonal = "multiplicative")),
          get_retail_test_rmse(hw(retail_train, seasonal = "additive", damped = TRUE)),
          get_retail_test_rmse(hw(retail_train, seasonal = "multiplicative", damped = TRUE)))

rmse_df <- data.frame(Model, RMSE) 

rmse_df %>%
  kable() %>%
  kable_styling()
```

**This was and interesting exercise.  I decided to run it through a buch of models, even some that didn't make sense given this data set.  Some of the more sophisticated approaches preformed less well on the test set than the seasonal naïve model.  In the end the `r rmse_df[rmse_df$RMSE == min(rmse_df$RMSE),]$Model` preformed the best on the test set.**

## Question 7.9 

For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?

```{r}
# Window doesn't return a ts object so we need to redefine
retail_train <- ts(as.vector(retail_ts), start=c(1982,4), end=c(2010,12), frequency = 12)
lambda <- BoxCox.lambda(retail_train)
bc_retail_train <- BoxCox(retail_train, lambda = lambda)
stl_retail_train <- stl(bc_retail_train, s.window="periodic") # Guessing on s.window setting based on documentation
stl_time_series <- data.frame(stl_retail_train$time.series)
sa_retail_train <- bc_retail_train - stl_time_series$seasonal

ets_retail_train <- forecast(sa_retail_train)
bc_retail_test <- BoxCox(retail_test, lambda = lambda)
accuracy(ets_retail_train, bc_retail_test)
```
