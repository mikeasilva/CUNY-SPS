---
title: "DATA 624 Project 1 Part A - ATM"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---


```{r setup, echo=FALSE, cache=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(lubridate)
library(kableExtra)
library(readxl)
library(fpp2)
library(forcats)
library(gridExtra)
options(scipen=999)
```

## Introduction

I am to forecast how much cash will be taken out of 4 different ATM machines for May 2010.  Data was provided for this project.  The cash is in hundreds of dollars.

## Data Exploration

I will begin by plotting the cash withdrawn by each ATM.

```{r}
atm <- read_excel("ATM624Data.xlsx") %>%
  mutate(DATE = as.Date(DATE, origin = "1899-12-30"),
         ATM = as.factor(ATM)) %>%
  data.frame()

atm %>%
  ggplot(aes(DATE, Cash, color=ATM)) +
  geom_line() +
  ggtitle("Cash Withdrawal by Date and ATM") +
  scale_color_brewer(palette = "Set1") +
  facet_wrap(.~ATM, ncol = 2) +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.title = element_blank())
```

ATM #4 is utilized at a higher rate than every other ATM in the data series.  There is an unusual spike in ATM #4.  According to a [2010 article in Time magazine](https://business.time.com/2010/10/07/more-thevies-are-making-total-atm-withdrawls/) an average sized ATM can hold $200,000.  So the outlier is an implausible figure.  There are some missing ATM labels in the data.  Let's remove ATM #4 and the missing label from the dataset and replot the visualization:

```{r}
atm %>%
  filter(ATM != "ATM4") %>%
  filter(!is.na(ATM)) %>%
  ggplot(aes(DATE, Cash, color=ATM)) +
  geom_line() +
  facet_wrap(.~ATM, ncol = 2) +
  ggtitle("Cash Withdrawal by Date and ATM") +
  scale_color_brewer(palette = "Set1") + 
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.title = element_blank())
```

It looks like ATM #3 was not utilized until recently.  ATM #1 and #2 seem very similar to each other.  It doesn't seem like there is much of a trend in the data.  Let's explore the distributions further with this visualization:

```{r}
atm %>%
  na.omit() %>%
  ggplot(aes(ATM, Cash, color = ATM)) +
  geom_boxplot() +
  ggtitle("Cash Withdrawal by ATM") +
  scale_color_brewer(palette = "Set1") +
  facet_wrap(.~ATM, nrow = 2, scales = "free") +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

atm %>%
  filter(!is.na(ATM)) %>%
  group_by(ATM) %>%
  summarise(Minimum = min(Cash, na.rm = T),
            `1st Qu.` = quantile(Cash, .25, na.rm = T),
            Mean = mean(Cash, na.rm = T),
            Median = median(Cash, na.rm = T),
            `3rd Qu.` = quantile(Cash, .75, na.rm = T),
            Maximum = max(Cash, na.rm = T),
            `NA's` = sum(is.na(Cash))) %>%
  kable() %>%
  kable_styling()
```

### Is there any Daily Effects?

There is a pattern in the time series but fluctuations are very tight.  I wonder if it is explained by the day of the week.  For example, Friday and Saturday might have heavier usage and a Tuesday night might be calm.  Let's examine this hypothesis:

```{r}
atm %>%
  na.omit() %>%
  mutate(`Day of Week` = recode(as.factor(wday(DATE)), "1" = "Sunday", "2" = "Monday", "3" = "Tuesday", "4"="Wednesday", "5"="Thursday", "6"="Friday","7"="Saturday"))  %>%
  ggplot(aes(`Day of Week`, Cash, color = `Day of Week`)) +
  geom_boxplot() +
  facet_grid(ATM ~ `Day of Week`, scales = "free") +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "none", axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank())
```

It looks like there is some varriation based on the day of the week.  Thursday looks to be less busy than the other days of the week.  There is so much variation between the 4 ATMs that each will be modeled seperately.

## A Quick Note on Process

I will clean up the data to produce the forecasts.  I will then explore some candidate modeling techniques.  Once I have a handful of candidates, I will prefrom a cross-validation on the models and get the RMSE.  The model that minimizes the RMSE during cross-validation will be selected as the model of choice.

## ATM #1

### Data Cleanup

I will begin ATM #1.  There are 3 missing observations that will need to be cleaned up.

```{r}
atm1 <- atm %>%
  filter(ATM == "ATM1")

atm1$clean_cash <- c(tsclean(ts(atm1$Cash, start = decimal_date(as.Date(min(atm1$DATE))), frequency = 365)))
```

Now that we have a complete data set I will create time series objects.  Since there is a weekly effect I will be using a frequency of 7.  This does mess up the dates in the plot however so please pay no reguard to them.

```{r}
ggtsplot <- function(ts, title) {
  # A ggplot2 version of tsdisplay(df)
  # Args:
  #    ts (Time-Series): The time series we want to plot
  #    title (str): The title of the graph
  grid.arrange(
    autoplot(ts) +
      ggtitle(title) +
      theme(axis.title = element_blank()),
    grid.arrange(
      ggAcf(ts) + ggtitle(element_blank()),
      ggPacf(ts) + ggtitle(element_blank()), ncol = 2)
    , nrow = 2)
}

atm1_ts <- ts(atm1$clean_cash, start = decimal_date(as.Date(min(atm1$DATE))), frequency = 7)
ggtsplot(atm1_ts, "ATM #1")
```

One readily observes the repeating peaks on every 7th lag.  There also seems to be an interesting negative correlation between the 1st and both the 3rd and the 5th lag.

### Model Creation 

#### STL Decomposition Models

I will try a couple of seasonal decomposition models.  I will set the seasonal window to 7 so it picks up the day of the week variation.  

```{r}
atm1_ts %>%
  stl(s.window = 7, robust = TRUE) %>%
  autoplot()
```

This seems promising to me.  I will do STL decomposition forecasts using both the ETS and ARIMA models and check their residual plots

#### STL + ETS

```{r}
h <- 31
atm1_stl_ets_fit <- atm1_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method="ets")
checkresiduals(atm1_stl_ets_fit)
```

This model seems to hold merit and should be taken under consideration.

#### STL + ARIMA

```{r}
atm1_stl_arima_fit <- atm1_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method="arima") 
checkresiduals(atm1_stl_arima_fit)
```

This model also preformed well.  It is definately a candidate for the cross-validation stage.

#### Holt-Winters

```{r}
atm1_hw_fit <- hw(atm1_ts, h = h)
checkresiduals(atm1_hw_fit)
```

This method did a fairly good job.  I will move it on to the next phase.

### Holt-Winters with Box Cox Adjustment

```{r}
atm1_lambda <- BoxCox.lambda(atm1_ts)
atm1_adj_hw_fit <- hw(atm1_ts, h = h, lambda = atm1_lambda)
checkresiduals(atm1_adj_hw_fit)
```

This seems to be a strong candidate.  We will see how if fares in the cross-validation.

#### ARIMA

```{r}
atm1_arima_fit <- auto.arima(atm1_ts)
checkresiduals(atm1_arima_fit)
```

This model looks like it is preforming well.  Let's see how all of them stack up.

### Cross Validation

In order to understand how well a model is likely to preform at predicting out of sample data, I will use the `tsCV` function and evaluate the models.  As prevously noted my goal is to minimize the RMSE.  First I will get the errors from the cross validation process, then I will compute the RMSE.

```{r}
get_rmse <- function(e) {
  sqrt(mean(e^2, na.rm = TRUE))
}

atm1_arima_forecast <- function(x, h) {
  forecast(Arima(x, order = c(0, 0, 1), seasonal = c(0, 1, 2)), h = h)
}

stl_ets_errors <- tsCV(atm1_ts, stlf, h = h, s.window = 7, robust = TRUE, method="ets")
stl_arima_errors <- tsCV(atm1_ts, stlf, h = h, s.window = 7, robust = TRUE, method="arima")
hw_errors <- tsCV(atm1_ts, hw, h = h)
adj_hw_errors <- tsCV(atm1_ts, hw, h = h, lambda = atm1_lambda)
arima_errors <- tsCV(atm1_ts, atm1_arima_forecast, h = h)

data.frame(Model = c("STL ETS", "STL ARIMA", "ARIMA", "Holt-Winters", "Adjusted Holt-Winters"),
           RMSE = c(get_rmse(stl_ets_errors[, h]), get_rmse(stl_arima_errors[, h]), get_rmse(arima_errors[, h]), get_rmse(hw_errors[, h]), get_rmse(adj_hw_errors[, h]))) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

It looks like the ARIMA model did the best job predicting on the out of sample data set.  

### Model Selection

I will select the ARIMA model to produce the forecast for ATM #1.  I have decided on this because it is the model that preformed the best in the cross validation.

## ATM #2

Now I will repeat the above process for ATM #2.  As this is a repeat I will not include as much explanatory text.

### Data Cleanup

There are 2 missing observations.  This will be cleaned up using the `tsclean` function again.

```{r}
atm2 <- atm %>%
  filter(ATM == "ATM2")

atm2$clean_cash <- c(tsclean(ts(atm2$Cash, start = decimal_date(as.Date(min(atm2$DATE))), frequency = 365)))

atm2_ts <- ts(atm2$clean_cash, start = decimal_date(as.Date(min(atm2$DATE))), frequency = 7)
ggtsplot(atm2_ts, "ATM #2")
```

Once again the ACF plot has the regular spikes on evry multiple of 7.

### Model Creation 

#### STL Decomposition Models

Again I will try a couple of seasonal decomposition models.  I will set the seasonal window to 7 so it picks up the day of the week variation.  

```{r}
atm2_ts %>%
  stl(s.window = 7, robust = TRUE) %>%
  autoplot()
```

This seems very similar to ATM #1.

#### STL + ETS

```{r}
atm2_stl_ets_fit <- atm2_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method="ets")
checkresiduals(atm2_stl_ets_fit)
```

This model preformed much better than the it did for the ATM #1 time series.  It will be interesting to see how it does in cross-validation

#### STL + ARIMA

```{r}
atm2_stl_arima_fit <- atm2_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method="arima") 
checkresiduals(atm2_stl_arima_fit)
```

Interesting.  The residuals have a bit more spread but the left tail is shorter than the STL+ETS model.

#### Holt-Winters

```{r}
atm2_hw_fit <- hw(atm2_ts, h = h)
checkresiduals(atm2_hw_fit)
```

This method did not preform as well as the others.  I will, however keep it in so I can compare it to ATM #1's statistics.

### Holt-Winters with Box Cox Adjustment

```{r}
atm2_lambda <- BoxCox.lambda(atm2_ts)
atm2_adj_hw_fit <- hw(atm2_ts, h = h, lambda = atm2_lambda)
checkresiduals(atm2_adj_hw_fit)
```

#### ARIMA

```{r}
atm2_arima_fit <- auto.arima(atm2_ts)
checkresiduals(atm2_arima_fit)
```

This model looks like it is preforming well.  Let's see how all of them stack up.

### Cross Validation

In order to understand how well a model is likely to preform at predicting out of sample data, I will use the `tsCV` function and evaluate the models.  As prevously noted my goal is to minimize the RMSE.  First I will get the errors from the cross validation process, then I will compute the RMSE.

```{r}
atm2_arima_forecast <- function(x, h) {
  forecast(Arima(x, order = c(2, 0, 2), seasonal = c(0, 1, 1)), h = h)
}

stl_ets_errors <- tsCV(atm2_ts, stlf, h = h, s.window = 7, robust = TRUE, method="ets")
stl_arima_errors <- tsCV(atm2_ts, stlf, h = h, s.window = 7, robust = TRUE, method="arima")
hw_errors <- tsCV(atm2_ts, hw, h = h)
adj_hw_errors <- tsCV(atm2_ts, hw, h = h, lambda = atm2_lambda)
arima_errors <- tsCV(atm2_ts, atm2_arima_forecast, h = h)

data.frame(Model = c("STL ETS", "STL ARIMA", "ARIMA", "Holt-Winters", "Adjusted Holt-Winters"),
           RMSE = c(get_rmse(stl_ets_errors[, h]), get_rmse(stl_arima_errors[, h]), get_rmse(arima_errors[, h]), get_rmse(hw_errors[, h]), get_rmse(adj_hw_errors[, h]))) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

Interesting.  It looks like the ARIMA model was the top preformer again.  I find it interesting that the RMSE is higher and more spread out for ATM #2.  ATM #1's RMSEs ranged from roughyl 30 to 39.  The RMSEs for ATM #2 ranged from 34 to 55.  There is more variability in this data that isn't captured by the model.

### Model Selection

As stated in the previous section I will use the ARIMA model to produce the forecast for ATM #2.

## ATM #3
