{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Discussion Assignment 3\n",
    "\n",
    "## Do Recommender Systems Reinforce Human Bias? \n",
    "\n",
    "I believe they do.  All recommender systems base their predictions on the ratings of previous users.  Missing data is inferred from those that provided data.  Recommender systems can consequently reinforce human bias.  To the degree that the user's are biased, they can be repeated.  Coupled with recommender evaluation being based on what people, the reinforced human bias could be counted as a positive thing.\n",
    "\n",
    "One concrete example of this process was the charge that [YouTube was radicalizing viewers](https://www.theatlantic.com/politics/archive/2018/03/youtube-extremism-and-the-long-tail/555350/).  A researcher watched political speeches by Donald Trump and Hillary Clinton and then let YouTube's autoplay run.  This effectively handed the controls over to the recommender system.  The result: videos quickly moved to the extremes of the political spectrum.  She concluded \n",
    "\n",
    "> Videos about vegetarianism led to videos about veganism. Videos about jogging led to videos about running ultramarathons. It seems as if you are never “hard core” enough for YouTube’s recommendation algorithm.\n",
    "\n",
    "> It promotes, recommends, and disseminates videos in a manner that appears to constantly up the stakes. Given its billion or so users, YouTube may be one of the most powerful radicalizing instruments of the 21st century.\n",
    "\n",
    "## Is there any hope?\n",
    "\n",
    "I do not believe this is hopeless.  I think it would take the great thought and action by the data scientist to protect the user against the recommender system.  One thing would be to follow the suggestions found in [this article](https://www.pcmag.com/news/how-to-build-an-ethical-algorithm) in building an ethical algorithm.  The other practice that could be used is to always make the user king.  \n",
    "\n",
    "The YouTube example went far off field because of the autoplay the next recommended video.  Leave the decision on what to play next in the hands of the user.  Don't assume they want it.  This basically argues for humility from the data scientist that the data might not predict what they user would like, despite our best efforts to do so."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
