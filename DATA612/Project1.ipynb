{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 612 Project 1 - Joke Recommender System\n",
    "\n",
    "By Mike Silva\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This recommender system provides our users with jokes that they will find funny.  By providing this content we will keep users engaged longer.\n",
    "\n",
    "### About the Jester Dataset\n",
    "\n",
    "For this project I will be using the [Jester dataset](http://eigentaste.berkeley.edu/dataset/).  It was created by Ken Goldberg at UC Berkley (Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001).\n",
    "\n",
    "Data files are in .zip format, when unzipped, they are in Excel (.xls) format.  The ratings are real values ranging from -10.00 to +10.00 (the value \"99\" corresponds to \"null\" meaning \"not rated\").  Each row is a user.  The first column gives the number of jokes rated by the user. The next 100 give the ratings for jokes 1 to 100.  I will only be the first data set that has data for users that have rated 36 or more jokes.\n",
    "\n",
    "### data612\n",
    "\n",
    "This notebook relies on the module I created for this class.  You can see the [data612 module here](https://github.com/mikeasilva/CUNY-SPS/blob/master/DATA612/data612.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:51.046368Z",
     "start_time": "2020-02-12T02:47:44.640065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24983, 101)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import data612\n",
    "\n",
    "# STEP 1 - DOWNLOAD THE DATA SET\n",
    "if not os.path.exists(\"jester_dataset_1_1.zip\"):\n",
    "    # We need to download it\n",
    "    response = requests.get(\"http://eigentaste.berkeley.edu/dataset/jester_dataset_1_1.zip\")\n",
    "    if response.status_code == 200:\n",
    "        with open(\"jester_dataset_1_1.zip\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "# STEP 2 - EXTRACT THE DATA SET\n",
    "if not os.path.exists(\"jester-data-1.xls\"):\n",
    "    with zipfile.ZipFile(\"jester_dataset_1_1.zip\",\"r\") as z:\n",
    "        z.extract(\"jester-data-1.xls\")\n",
    "# STEP 3 - READ ING THE DATA\n",
    "# The data is a continous rating scale from -10 to 10.  99 is used if a user hasn't rated a joke.\n",
    "# The data does not have a header.  Using the column numbers works great.\n",
    "df = pd.read_excel(\"jester-data-1.xls\",  header=None, na_values = 99)\n",
    "# We should have a 24,983 X 101 data frame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first column (0) is the number of jokes rated by the user we can drop that so we are only left with the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:51.067312Z",
     "start_time": "2020-02-12T02:47:51.047406Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop([0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break into Training and Test Sets\n",
    "\n",
    "Now that we have data we will break it into training and test sets.  There are 1,810,455 ratings in the data set so this will take some time.  I am going to \"cache\" the output of this cell by saving it to disk and reading it in if it is present for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:51.435199Z",
     "start_time": "2020-02-12T02:47:51.070304Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"project_1_train_df.csv\") or not os.path.exists(\"project_1_test_df.csv\"):\n",
    "    train_df, test_df = data612.train_test_split(df)\n",
    "    train_df.to_csv(\"project_1_train_df.csv\", index = False)\n",
    "    test_df.to_csv(\"project_1_test_df.csv\", index = False)\n",
    "else:\n",
    "    train_df = pd.read_csv(\"project_1_train_df.csv\")\n",
    "    test_df = pd.read_csv(\"project_1_test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Average Rating\n",
    "\n",
    "Now that we have a training set we need calculate the raw average (mean) rating for every user-item combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:51.477553Z",
     "start_time": "2020-02-12T02:47:51.436162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8801402409891428"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_avg = train_df.sum(numeric_only=True).sum() / train_df.count().sum(axis = 0)\n",
    "raw_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T14:57:26.088634Z",
     "start_time": "2020-01-30T14:57:26.031406Z"
    }
   },
   "source": [
    "This almost one mean rating indicates most of the jokes are not very funny.\n",
    "\n",
    "## Calculate the RMSE \n",
    "\n",
    "Now I will calculate the RMSE for raw average for both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:54.361663Z",
     "start_time": "2020-02-12T02:47:51.478518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.236313400381647"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_RMSE = data612.get_RMSE(train_df, raw_avg)\n",
    "train_df_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:57.110391Z",
     "start_time": "2020-02-12T02:47:54.362663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.234060383407136"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_RMSE = data612.get_RMSE(test_df, raw_avg)\n",
    "test_df_RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Biases\n",
    "\n",
    "We now can calculate the user and item biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:57.142410Z",
     "start_time": "2020-02-12T02:47:57.111388Z"
    }
   },
   "outputs": [],
   "source": [
    "user_bias_train_df, item_bias_train_df = data612.get_biases(train_df, raw_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Predictions\n",
    "\n",
    "Now we can make our baseline predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:47:59.219167Z",
     "start_time": "2020-02-12T02:47:57.143738Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_predictions_df = data612.get_baseline_predictions(raw_avg, user_bias_train_df, item_bias_train_df)\n",
    "baseline_predictions_df = data612.get_valid_jester_predictions(baseline_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Revisited\n",
    "\n",
    "Now we can see how well the baseline predictions preformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:48:02.068878Z",
     "start_time": "2020-02-12T02:47:59.219167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.280588684895016"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data612.get_RMSE(train_df, baseline_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T02:48:04.765821Z",
     "start_time": "2020-02-12T02:48:02.069849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3586750167649"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data612.get_RMSE(test_df, baseline_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Findings\n",
    "\n",
    "The baseline predictions that take into account the user and item biases preform a little better for the test set.  The RMSE decreased from roughly 5.2 to 4.3 which is a 20% change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
