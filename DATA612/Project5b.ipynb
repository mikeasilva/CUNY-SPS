{"cells":[{"cell_type":"markdown","source":["By Mike Silva\n\n## Introduction\n\nThe goal of this project is to practice beginning to work with a distributed recommender system.  For this project I will be using data scrapped from BoardGameGeek.com (BGG).  This is part two of the project.  Previously I have built and tested the execution time for building an ALS based model on my laptop.  This is duplicating this but on a Spark platform.\n\n### About the BGG Dataset\nThe BoardGameGeek dataset was collected by myself by scrapping data from the API that forms the backend of [BoardGameGeek's website](https://boardgamegeek.com/). Data scrapping in ongoing but this particular data set has over 1.9 million ratings (implicit and explicit) for about 88,000 games by 219,000 users. I have previously exported the ratings from the SQLite database and uploaded them to the [Databricks Community Platform](https://community.cloud.databricks.com/).\n\n## Data Wrangling\n\nThis is the same process used in the first part of this project.  It will fill in the implicit ratings with the explicit values using the same distribution.  For a more through explanation of the method I would refer the reader to [Part A's notebook on GitHub](https://github.com/mikeasilva/CUNY-SPS/blob/master/DATA612/Project5a.ipynb)"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\nimport seaborn as sns\nimport time\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n\nmy_seed = 42\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\n# Read in the raw data\ndf = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", \",\").load(\"/FileStore/tables/bgg_ratings.csv\")\ndf = df.select(\"*\").toPandas()\n\n# Wrangle the ratings\ndf[\"rounded_rating\"] = df[\"rating\"].round().astype(int)\n\n# Extract explicit ratings\nis_an_explicit_rating = (df['rating_tstamp'].notnull()) | (df['rounded_rating'] > 0)\nis_an_explicit_rating = df['rounded_rating'] > 0\nexplicit_rating = df[is_an_explicit_rating]\n\n# Get the values to fill in the implicit ratings\nexplicit_rating = explicit_rating.assign(n= 1)\nfill_with = explicit_rating[explicit_rating.rounded_rating > 6].groupby(\"rounded_rating\")[[\"n\"]].agg(\"sum\").reset_index()\n\n# Get the probabilities to fill the ratings\ntemp = dict()\ntotal = 0\n\nfor item in fill_with.to_dict(\"records\"):\n    temp[item[\"rounded_rating\"]] = item[\"n\"]\n    total += item[\"n\"]\nchoices = list(temp.keys())\nprobs = [val / total for rating, val in temp.items()]\n\n# Fill implicit ratings\nimplicit_rating = df[~is_an_explicit_rating]\nimplicit_rating = implicit_rating.assign(rounded_rating = np.random.choice(choices, len(implicit_rating.index), p=probs))\n\n# Combine the explicit and implicit data frames\ncols = [\"user_id\", \"item_id\", \"rounded_rating\"]\nbgg_ratings = pd.concat([explicit_rating[cols], implicit_rating[cols]])\n\n# Remove sparse items\ncounts_by_item = bgg_ratings.assign(n = 1).groupby(\"item_id\")[[\"n\"]].agg(\"sum\").reset_index()\nitem_filter = counts_by_item[counts_by_item.n >= 25][\"item_id\"].tolist()\nbgg_ratings = bgg_ratings[bgg_ratings.item_id.isin(item_filter)].reset_index(drop=True)\n\n# Remove sparse users\ncounts_by_user = bgg_ratings.assign(n= 1).groupby(\"user_id\")[[\"n\"]].agg(\"sum\").reset_index()\nuser_filter = counts_by_user[counts_by_user.n >= 10][\"user_id\"].tolist()\nbgg_ratings = bgg_ratings[bgg_ratings.user_id.isin(user_filter)].reset_index(drop=True)\n\nbgg_ratings = sqlContext.createDataFrame(bgg_ratings)\nbgg_ratings.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+--------------+\nuser_id|item_id|rounded_rating|\n+-------+-------+--------------+\n    987|      3|             9|\n    940|      3|             9|\n    607|      3|             8|\n   8972|      3|             8|\n   8973|      3|             8|\n   8974|      3|             8|\n    960|      3|             8|\n    948|      3|             8|\n    936|      3|             8|\n   8975|      3|             8|\n   1499|      3|             7|\n   8976|      3|             7|\n    181|      3|             7|\n   8977|      3|             7|\n   8978|      3|             7|\n    127|      3|             7|\n   4754|      3|             7|\n   8979|      3|             6|\n   8980|      3|             6|\n   8981|      3|             6|\n+-------+-------+--------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["Now that I have the data in Spark, I will replicate the same process I used on my local laptop.  I will train the model using a vanilla ALS algorythm.  I will repeat the process 5 times and compute an average off of those five runs on all evaluation metrics."],"metadata":{}},{"cell_type":"code","source":["def spark_k_fold_als(algo, k, data):\n  total_rmse = 0\n  train_time = 0\n  test_time = 0\n  rmse_time = 0\n  for i in range(0, k):\n    (training_set, test_set) = data.randomSplit([0.8, 0.2])\n    # Train the models on the training set\n    train_start_time = time.time()\n    als_model = algo.fit(training_set)\n    train_time += time.time() - train_start_time\n    # Get the predictions on the test set\n    test_start_time = time.time()\n    algo_predictions = als_model.transform(test_set)\n    test_time += time.time() - test_start_time\n    # Add in the RMSE\n    rmse_start_time = time.time()\n    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rounded_rating\", predictionCol=\"prediction\")\n    total_rmse += evaluator.evaluate(algo_predictions)\n    rmse_time += time.time() - test_start_time\n  average_rmse = total_rmse / k\n  average_train_time = train_time / k\n  average_test_time = test_time / k\n  average_rmse_time = rmse_time / k\n  return (average_rmse, average_train_time, average_test_time, average_rmse_time)\n\n\nvanilla_als = ALS(userCol = \"user_id\", itemCol=\"item_id\", ratingCol=\"rounded_rating\", coldStartStrategy=\"drop\", nonnegative=True)\n  \naverage_rmse, average_train_time, average_test_time, average_rmse_time = spark_k_fold_als(vanilla_als, 5, bgg_ratings)\nprint(\"Training Model: %s seconds\" % (average_train_time))\nprint(\"Predictions: %s seconds\" % (average_test_time))\nprint(\"Evaluation: %s seconds\" % (average_rmse_time))\nprint(\"RMSE: %s\" % (average_rmse))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training Model: 56.30785298347473 seconds\nPredictions: 0.035971784591674806 seconds\nEvaluation: 66.98323187828063 seconds\nRMSE: 1.4212333949235902\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Conclusion\n\nI have preformed a 5 fold cross validation of a ALS recommender system model on both my laptop and databricks community edition.  I gathered the RMSE and execution times for the 3 stages of the process (model training, test set predictions, prediction evaluation).  Here's the results:\n\n| Metric               | My Laptop          | Databricks           |\n|----------------------|--------------------|----------------------|\n| RMSE                 | 1.297831628696796  | 1.4212333949235902   |\n| Training (seconds)   | 3.6788469791412353 | 56.30785298347473    |\n| Prediction (seconds) | 1.3248661518096925 | 0.035971784591674806 |\n| Evaluation (seconds) | 1.493586826324463  | 66.98323187828063    |\n| Total Execution Time | 31.5 seconds       | 10.28 minutes        |\n\nThere are some aspects that my computer out preformed Spark, most notably in overall execution time, training, and evaluation.  However Spark outpreformed the prediction step.  The prediction is lightning fast.\n\nIt is interesting to me that there is that much of a difference in the RMSE.  I thought it would have been less.\n\nI could see that this difference would be increasingly important as the data set gets larger.  If a new set of data (i.e. a user's session) flows in, getting quick predictions would be paramount.  There is also a point where the data will be too large for my laptop's resources.  That is definately when I would need to move to the distributed platform."],"metadata":{}}],"metadata":{"name":"Project5b","notebookId":3727236862064049},"nbformat":4,"nbformat_minor":0}
