---
title: "kNN Implementation"
author: "Raman Kannan"
date: "8/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

We will continue with Instance based methods. These algorithms do NOT compute
probability distribution.

kNN works as follows

compute the distance between the given instance and all the known instance.
Sort by distance and assign the most frequent class of the k nearest neighbors.


```{r}
euclideanDist <- function(a, b){
  d = 0
  for(i in c(1:(length(a)) ))
  {
    d = d + (a[[i]]-b[[i]])^2
  }
  d = sqrt(d)
  return(d)
}
```
Let us write another function to iterate over the test set 
and over the training set for each observation in the test set.
```{r}
knn_predict2 <- function(test_data, train_data, k_value, labelcol){
  #print(k_value)
  #print(labelcol)
  pred <- c()  #empty pred vector 
  #LOOP-1
  for(i in c(1:nrow(test_data))){   #looping over each record of test data
    eu_dist =c()          #eu_dist & eu_char empty  vector
    eu_char = c()
    good = 0              #good & bad variable initialization with 0 value
    bad = 0
    
    #LOOP-2-looping over train data 
    for(j in c(1:nrow(train_data))){
      
      #adding euclidean distance b/w test data point and train data to eu_dist vector
      eu_dist <- c(eu_dist, euclideanDist(test_data[i,-c(labelcol)], train_data[j,-c(labelcol)]))
      
      #adding class variable of training data in eu_char
      eu_char <- c(eu_char, as.character(train_data[j,][[labelcol]]))
      #print(i,j,as.character(train_data[j,][[labelcol]])))
    }
    
    eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
    
    eu <- eu[order(eu$eu_dist),]       #sorting eu dataframe to gettop K neighbors
   # print(k_value)
    eu <- eu[1:k_value,]               #eu dataframe with top K neighbors
    
    #head(eu[1:k_value,])
    tbl.sm.df<-table(eu$eu_char)
    cl_label<-  names(tbl.sm.df)[[as.integer(which.max(tbl.sm.df))]]
    
    pred <- c(pred, cl_label)
  }
  return(pred) #return pred vector
}
```
Let us also write a utility function for accuracy.
```{r}
accuracy <- function(test_data,labelcol,predcol){
  correct = 0
  for(i in c(1:nrow(test_data))){
    if(test_data[i,labelcol] == test_data[i,predcol]){ 
      correct = correct+1
    }
  }
  accu = (correct/nrow(test_data)) * 100  
  return(accu)
}
```
Now let us load and prepare training/test set from the heart dataset.

We will call the predict function we wrote above.
```{r}
#load data
file<-'heart.csv'
heart<-read.csv(file,head=T,sep=',',stringsAsFactors=F)
head(heart)
catheart<-heart[,c(2,3,6,7,9,11,12,13,14)]

knn.df<-catheart
labelcol <- 9 # for iris it is the fifth col 
predictioncol<-labelcol+1
# create train/test partitions
set.seed(43)
n<-nrow(knn.df)
index <- sample(n)
knn.df<- knn.df[index,] # randomize

train.df <- knn.df[1:as.integer(0.7*n),]
test.df <- knn.df[as.integer(0.7*n +1):n,]


K<-3 # number of neighbors to determine the class

predictions <- knn_predict2(test.df, train.df, K,labelcol) #calling knn_predict()

```

```{r}
knn_predict3 <- function(test_data, train_data, k_value, labelcol){
  pred <- c() # Initialize an empty vector to hold the predictions
  eu_char <- train_data[,labelcol] # Get the class labels from the training data
  b <- train_data[,-c(labelcol)] # Drop the class label from the training data
  for(i in c(1:nrow(test_data))){
    a <- test_data[i,-c(labelcol)] # Pull the row from the test data
    eu_dist <- euclideanDist(a, b) # Calculate the euclidean distance between training data and test row
    eu <- data.frame(eu_char, eu_dist) # Put the distance and the label in a data frame
    eu <- eu[order(eu$eu_dist),] # Sort it by the euclidean distance
    eu <- eu[1:k_value,] # Subset the first k rows
    tbl.sm.df<-table(eu$eu_char) # Create frequency table
    pred <-  c(pred, names(tbl.sm.df)[[as.integer(which.max(tbl.sm.df))]]) # Add in the predicted class (highest frequency) to the predictions vector
  }
  return(pred) # Return the predictions
}
predictions2 <- knn_predict3(test.df, train.df, K,labelcol)
```

```{r}
start_time <- Sys.time()
n_tests <- 100
for (i in 1:n_tests){
 knn_predict2(test.df, train.df, K,labelcol)
}
end_time <- Sys.time()
difftime(end_time, start_time)
```

```{r}
start_time <- Sys.time()
for (i in 1:n_tests){
 knn_predict3(test.df, train.df, K,labelcol)
}
end_time <- Sys.time()
difftime(end_time, start_time)
```
Computing ROCR and AUC is some what non-trivial as kNN do not compute probabilities
and results in unreliable ROCR plots.
```{r}
test.df[,predictioncol] <- predictions #
print(paste('accuracy=',accuracy(test.df,labelcol,predictioncol)))
```
Now let us label the test set using standard kNN implementation from class
package.
```{r}
library(class)
library(ROCR)

train.df <- knn.df[1:as.integer(0.7*n),]
test.df <- knn.df[as.integer(0.7*n +1):n,]

cl<-factor(train.df[,9])
knnPred<-knn(train.df[,-c(labelcol)],test.df[,-c(labelcol)], cl, k = 3, prob=TRUE)
prob<-attr(knnPred,"prob")
kp<-prediction(prob,test.df[,labelcol])
AUC<-performance(kp,"auc")
#perf <- performance(pred,"tpr","fpr")
#plot(perf,colorize=TRUE)
pred_knn<-performance(kp,"tpr","fpr")
```
```{r echo=FALSE}
plot(pred_knn,avg="threshold",colorize=T,lwd=3,main="ROCR knn")
text(0.8,0.2,paste("AUC=",round(AUC@y.values[[1]],4),sep=''))
```
```{r echo=FALSE}
TBL<-table(test.df[,labelcol],knnPred)
print(paste("accuracy=",sum(diag(TBL))/sum(TBL)))
```

