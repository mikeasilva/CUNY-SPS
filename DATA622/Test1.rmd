---
title: "Test 1"
author: "Mike Silva"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Use the dataset you used for HW-1 (Blue/Black)
# 
# (A) Run Bagging (ipred package)   
# 
#   -- sample with replacement
# 
#   -- estimate metrics for a model
# 
#   -- repeat as many times as specied and report the average
# 
# (B) Run LOOCV (jacknife) for the same dataset
# 
# --- iterate over all points
# 
#      -- keep one observation as test
# 
#     -- train using the rest of the observations
# 
#     -- determine test metrics
# 
#     -- aggregate the test metrics
# 
# end of loop
# 
# find the average of the test metric(s)
# 
# Compare (A), (B) above with the results you obtained in HW-1  and write 3 sentences explaining the
# 
# observed difference.
# 
# You can implement it in java/spark, python or R.
# 
# Submit RMD or PDF and the python/R scripts.
# 
# --
# 
# i would recommend that you implement it from scratch but you may use any package. It is not a requirement.
# 
# bagging is available in ipred package in R.
```
This test is an extension of the work done in Homework 1.  I will be extending it with bagging and leave one out cross-validation (LOOCV), and adding in the results into the evaluation of the algorithms.

## Preliminaries

### Load Packages

```{r}
library(dplyr)
library(tidyr)
library(kableExtra)
library(caret)
library(e1071)
library(class)
library(ROCR)
```

### Read in the data

```{r}
df <- read.csv("https://github.com/mikeasilva/CUNY-SPS/raw/master/DATA622/HW1.csv", stringsAsFactors = TRUE)
```

## Data Prep

Since we will be using KNN we will need to normalize `X`.  In order to get a fair comparison, we will used the same processed data to see how the algorithms preform.

```{r}
normalize <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}

adjusted_df <- df
adjusted_df$X <- normalize(adjusted_df$X)
```

We also need to transform `Y` into dummy variables.

```{r}
adjusted_df <- adjusted_df %>%
  mutate(Y_a = ifelse(Y == "a", 1, 0),
         Y_b = ifelse(Y == "b", 1, 0),
         Y_c = ifelse(Y == "c", 1, 0),
         Y_d = ifelse(Y == "d", 1, 0),
         Y_e = ifelse(Y == "e", 1, 0)) %>%
  select(-Y)
```

In order to evaluate the ability of the of the algorithms to generalize, the data needs to be divided into training and test sets.  Since there is only `r nrow(df)` observations, I will be reserving 20% for the evaluation.

```{r}
set.seed(123)
in_training_set <- createDataPartition(adjusted_df$label, p = 0.8, list = FALSE, times = 1)
training_df <- adjusted_df[in_training_set,]
test_df  <- adjusted_df[-in_training_set,]
# These subsets will help with training and evaluation
training_df_without_label <- training_df %>% select(-label)
test_df_without_label  <- test_df %>% select(-label)
training_df_label <- training_df$label
test_df_label <- test_df$label
```

## Helper Functions

I will use the following helper functions.  This first one takes a logistic regression model and returns predictions for the `data` data.frame

```{r}
get_lr_yhat <- function(lr_model, data, label_col_name, threshold = 0.5){
  data_levels <- levels(data[[label_col_name]])
  cols_to_keep <- label_col_name != names(data)
  data <- data[,cols_to_keep]
  lr_yhat <- predict(lr_model, data, type = "response")
  lr_yhat <- as.factor(ifelse(lr_yhat <= threshold, data_levels[1], data_levels[2]))
  return(lr_yhat)
}
```

The second will take the the ground truth labels and predicted labels and create metrics for evaluation.

```{r}
evaluate_algo <- function(ground_truth, yhat, algo){
  cm <- confusionMatrix(table(ground_truth, yhat))
  cm_table <- cm$table
  tpr <- cm_table[[1]] / (cm_table[[1]] + cm_table[[4]])
  fnr <- 1 - tpr
  fpr <- cm_table[[3]] / (cm_table[[3]] + cm_table[[4]])
  tnr <- 1 - fpr
  accuracy <- cm$overall[[1]]
  for_auc <- prediction(c(yhat), ground_truth)
  auc <- performance(for_auc, "auc")
  auc <- auc@y.values[[1]]
  return(data.frame(Algo = algo, AUC = auc, ACCURACY = accuracy, TPR = tpr, FPR = fpr, TNR = tnr, FNR = fnr))
}
```

Now for the extensions for the test.  This third function will be used to create a bootstrap sample.  I will also make the assumption that we will use 100 boostrap samples in evaluating the performance of bagging.

```{r}
get_bootstrap_sample <- function(data, bootstrap_proportion = 0.6, sample_with_replacement = TRUE){
  n_bootstrap_observations <- round(nrow(data) * bootstrap_proportion, 0)
  return(data[sample(nrow(data), n_bootstrap_observations, replace = sample_with_replacement),])
}

n_bags <- 100
```

I will also need a way to aggregate/average the metrics

```{r}
average_metrics <- function(metrics, algo){
  avg_metrics <- metrics %>%
  select(-Algo) %>%
  colMeans(na.rm = TRUE) %>%
  t() %>%
  data.frame("Algo" = c(algo), .)
  return(avg_metrics)
}
```

## Baseline Models

First I will train the models following what was previously done in HW1 and collect the metrics on both the training and test data sets

### Logistic Regression

```{r}
lr_model <- glm(label ~ ., data = training_df, family = "binomial")
summary(lr_model)
```
#### Capacity to Learn

**The sample data are too small (`r nrow(training_df)`) to have any confidence in the Logistic Regression model, so I will be throwing this one out.  Bagging and LOOCV will not resolve this issue.**

```{r}
capacity_to_learn <- data.frame("Algo" = c("LR", "LR with Bagging", "LR with LOOCV"), AUC=c(NA), ACCURACY=c(NA), TPR=c(NA), FPR=c(NA), TNR=c(NA), FNR=c(NA))
```

#### Capacity to Generalize

**Again the sample data are too small to use to make meaningful inferences using Logistic Regression.  Bagging and LOOCV will not help and will be excluded from the metrics.**

```{r}
capacity_to_generalize <- data.frame("Algo" = c("LR", "LR with Bagging", "LR with LOOCV"), AUC=c(NA), ACCURACY=c(NA), TPR=c(NA), FPR=c(NA), TNR=c(NA), FNR=c(NA))
```

### Naive Bayes

I will repeat the same process for Naive Bayes.

#### Capacity to Learn

```{r}
nb_model <- naiveBayes(label ~ ., data = training_df)
training_nb_yhat <- predict(nb_model, training_df_without_label)
capacity_to_learn <- rbind(capacity_to_learn, evaluate_algo(training_df_label, training_nb_yhat, "NB"))
```

#### Capacity to Generalize

```{r}
nb_yhat <- predict(nb_model, test_df_without_label)
capacity_to_generalize <- rbind(capacity_to_generalize, evaluate_algo(test_df_label, nb_yhat, "NB"))
```

### KNN

Finally I will repeat the process for KNN where k=3 and k=5

#### Capacity to Learn

```{r}
training_knn3_yhat <- knn(training_df_without_label, training_df_without_label, training_df_label, k = 3)
capacity_to_learn <- rbind(capacity_to_learn, evaluate_algo(training_df_label, training_knn3_yhat, "KNN3"))

training_knn5_yhat <- knn(training_df_without_label, training_df_without_label, training_df_label, k = 5)
capacity_to_learn <- rbind(capacity_to_learn, evaluate_algo(training_df_label, training_knn5_yhat, "KNN5"))
```

#### Capacity to Generalize

```{r}
knn3_yhat <- knn(training_df_without_label, test_df_without_label, training_df_label, k = 3)
capacity_to_generalize <- rbind(capacity_to_generalize, evaluate_algo(test_df_label, knn3_yhat, "KNN3"))

knn5_yhat <- knn(training_df_without_label, test_df_without_label, training_df_label, k = 5)
capacity_to_generalize <- rbind(capacity_to_generalize, evaluate_algo(test_df_label, knn5_yhat, "KNN5"))
```

## Models With Bagging

Now to try bagging the Naive Bayes, and KNN models.  I will use the models trained on the bootstrap samples to predict the test data and collect metrics on the capacity to learn and generalize, which will be aggregated.

```{r}
nb_bag <- list()
for (i in 1:n_bags){
  bag_df <- get_bootstrap_sample(training_df)
  bag_df_without_label <- bag_df %>% select(-label)
  bag_df_label <- bag_df$label
  
  # Train the Naive Bayes Model
  bag_nb_model <- naiveBayes(label ~ ., data = bag_df)
  training_bag_nb_yhat <- predict(bag_nb_model, bag_df_without_label)
  ## Evaluate the capacity to learn
  bag_capacity_to_learn <- evaluate_algo(bag_df_label, training_bag_nb_yhat, paste("NB Bag", i))
  if(exists("nb_bag_capacity_to_learn")){
    nb_bag_capacity_to_learn <- rbind(nb_bag_capacity_to_learn, bag_capacity_to_learn)
  } else {
    nb_bag_capacity_to_learn <- bag_capacity_to_learn
  }
  ## Evaluate the capacity to generalize
  bag_nb_test_yhat <- predict(bag_nb_model, test_df_without_label)
  bag_capacity_to_generalize <- evaluate_algo(test_df_label, bag_nb_test_yhat, paste("NB Bag", i))
  if(exists("nb_bag_capacity_to_generalize")){
    nb_bag_capacity_to_generalize <- rbind(nb_bag_capacity_to_generalize, bag_capacity_to_generalize)
  } else {
    nb_bag_capacity_to_generalize <- bag_capacity_to_generalize
  }
  
  # Train the KNN3 Model
  ## Evaluate the capacity to learn
  training_bag_knn3_yhat <- knn(bag_df_without_label, bag_df_without_label, bag_df_label, k = 3)
  bag_capacity_to_learn <- evaluate_algo(bag_df_label, training_bag_knn3_yhat, paste("KNN3 Bag", i))
  if(exists("knn3_bag_capacity_to_learn")){
    knn3_bag_capacity_to_learn <- rbind(knn3_bag_capacity_to_learn, bag_capacity_to_learn)
  } else {
    knn3_bag_capacity_to_learn <- bag_capacity_to_learn
  }
  ## Evaluate the capacity to generalize
  bag_knn3_test_yhat <- knn(bag_df_without_label, test_df_without_label, bag_df_label, k = 3)
  bag_capacity_to_generalize <- evaluate_algo(test_df_label, bag_knn3_test_yhat, paste("KNN3 with Bagging", i))
  if(exists("knn3_bag_capacity_to_generalize")){
    knn3_bag_capacity_to_generalize <- rbind(knn3_bag_capacity_to_generalize, bag_capacity_to_generalize)
  } else {
    knn3_bag_capacity_to_generalize <- bag_capacity_to_generalize
  }
  
  # Train the KNN5 Model
  ## Evaluate the capacity to learn
  training_bag_knn5_yhat <- knn(bag_df_without_label, bag_df_without_label, bag_df_label, k = 5)
  bag_capacity_to_learn <- evaluate_algo(bag_df_label, training_bag_knn5_yhat, paste("KNN5 with Bagging", i))
  if(exists("knn5_bag_capacity_to_learn")){
    knn5_bag_capacity_to_learn <- rbind(knn5_bag_capacity_to_learn, bag_capacity_to_learn)
  } else {
    knn5_bag_capacity_to_learn <- bag_capacity_to_learn
  }
  ## Evaluate the capacity to generalize
  bag_knn5_test_yhat <- knn(bag_df_without_label, test_df_without_label, bag_df_label, k = 5)
  bag_capacity_to_generalize <- evaluate_algo(test_df_label, bag_knn5_test_yhat, paste("KNN5 with Bagging", i))
  if(exists("knn5_bag_capacity_to_generalize")){
    knn5_bag_capacity_to_generalize <- rbind(knn5_bag_capacity_to_generalize, bag_capacity_to_generalize)
  } else {
    knn5_bag_capacity_to_generalize <- bag_capacity_to_generalize
  }
  
}
```

Now to average the metrics:

```{r}
# Naive Bayes
capacity_to_learn <- rbind(capacity_to_learn, average_metrics(nb_bag_capacity_to_learn, "NB with Bagging"))
capacity_to_generalize <- rbind(capacity_to_generalize, average_metrics(nb_bag_capacity_to_generalize, "NB with Bagging"))

# KNN3
capacity_to_learn <- rbind(capacity_to_learn, average_metrics(knn3_bag_capacity_to_learn, "KNN3 with Bagging"))
capacity_to_generalize <- rbind(capacity_to_generalize, average_metrics(knn3_bag_capacity_to_generalize, "KNN3 with Bagging"))

# KNN5
capacity_to_learn <- rbind(capacity_to_learn, average_metrics(knn5_bag_capacity_to_learn, "KNN5 with Bagging"))
capacity_to_generalize <- rbind(capacity_to_generalize, average_metrics(knn5_bag_capacity_to_generalize, "KNN5 with Bagging"))
```

## Models With LOOCV

Now to examine how the models preform using LOOCV

```{r}
training_loocv_nb_yhat <- c()
training_loocv_knn3_yhat <- c()
training_loocv_knn5_yhat <- c()
loocv_test_label <- c()

for (i in 1:nrow(training_df)){
  # Leave One Out
  loocv_test <- training_df[i,]
  loocv_test_without_label <- loocv_test %>% select(-label)
  loocv_test_label <- c(loocv_test_label, loocv_test$label)
  loocv_training_df <- training_df[-c(i),]
  loocv_training_df_without_label <- loocv_training_df %>% select(-label)
  loocv_training_df_label <- loocv_training_df$label
  
  # Train the Naive Bayes Model
  loocv_nb_model <- naiveBayes(label ~ ., data = loocv_training_df)
  ## Evaluate the capacity to learn
  ### Note: This will be done once we have gone over the whole training data set
  training_loocv_nb_yhat <- c(training_loocv_nb_yhat, predict(loocv_nb_model, loocv_test_without_label))
  ## Evaluate the capacity to generalize
  loocv_nb_test_yhat <- predict(loocv_nb_model, test_df_without_label)
  loocv_capacity_to_generalize <- evaluate_algo(test_df_label, loocv_nb_test_yhat, paste("NB with LOOCV", i))
  if(exists("nb_loocv_capacity_to_generalize")){
    nb_loocv_capacity_to_generalize <- rbind(nb_loocv_capacity_to_generalize, loocv_capacity_to_generalize)
  } else {
    nb_loocv_capacity_to_generalize <- loocv_capacity_to_generalize
  }
  
  # Train the KNN3 Model
  ## Evaluate the capacity to learn
  ### Note: This will be done once we have gone over the whole training data set
  training_loocv_knn3_yhat <- c(training_loocv_knn3_yhat, knn(loocv_training_df_without_label, loocv_test_without_label, loocv_training_df_label, k = 3))
  ## Evaluate the capacity to generalize
  loocv_knn3_test_yhat <- knn(loocv_training_df_without_label, test_df_without_label, loocv_training_df_label, k = 3)
  loocv_capacity_to_generalize <- evaluate_algo(test_df_label, loocv_knn3_test_yhat, paste("KNN3 with LOOCV", i))
  if(exists("knn3_loocv_capacity_to_generalize")){
    knn3_loocv_capacity_to_generalize <- rbind(knn3_loocv_capacity_to_generalize, loocv_capacity_to_generalize)
  } else {
    knn3_loocv_capacity_to_generalize <- loocv_capacity_to_generalize
  }
  
  # Train the KNN5 Model
  ## Evaluate the capacity to learn
  ### Note: This will be done once we have gone over the whole training data set
  training_loocv_knn5_yhat <- c(training_loocv_knn5_yhat, knn(loocv_training_df_without_label, loocv_test_without_label, loocv_training_df_label, k = 5))
  ## Evaluate the capacity to generalize
  loocv_knn5_test_yhat <- knn(loocv_training_df_without_label, test_df_without_label, loocv_training_df_label, k = 5)
  loocv_capacity_to_generalize <- evaluate_algo(test_df_label, loocv_knn5_test_yhat, paste("KNN5 with LOOCV", i))
  if(exists("knn5_loocv_capacity_to_generalize")){
    knn5_loocv_capacity_to_generalize <- rbind(knn5_loocv_capacity_to_generalize, loocv_capacity_to_generalize)
  } else {
    knn5_loocv_capacity_to_generalize <- loocv_capacity_to_generalize
  }

}
```

Now to average the metrics:

```{r}
# Naive Bayes
capacity_to_learn <- rbind(capacity_to_learn, evaluate_algo(loocv_test_label, training_loocv_nb_yhat, "NB with LOOCV"))
capacity_to_generalize <- rbind(capacity_to_generalize, average_metrics(nb_loocv_capacity_to_generalize, "NB with LOOCV"))

# KNN3
capacity_to_learn <- rbind(capacity_to_learn, evaluate_algo(loocv_test_label, training_loocv_knn3_yhat, "KNN3 with LOOCV"))
capacity_to_generalize <- rbind(capacity_to_generalize, average_metrics(knn3_loocv_capacity_to_generalize, "KNN3 with LOOCV"))

# KNN5
capacity_to_learn <- rbind(capacity_to_learn, evaluate_algo(loocv_test_label, training_loocv_knn5_yhat, "KNN5 with LOOCV"))
capacity_to_generalize <- rbind(capacity_to_generalize, average_metrics(knn5_loocv_capacity_to_generalize, "KNN5 with LOOCV"))
```


## Summary and Analysis

We have evaluated `r nrow(capacity_to_learn)` algorithm's capacity to learn and generalize.  The following tables summarize their performance: 

**Table 1. Capacity to Learn**
```{r, echo=FALSE}
capacity_to_learn %>% arrange(Algo) %>% kable() %>%  kable_styling()
```

**Table 2. Capacity to Generalize**
```{r, echo=FALSE}
capacity_to_generalize %>% arrange(Algo) %>% kable() %>% kable_styling()
```

### Discussion

```{r}
ggplot() + geom_density(aes(x=ACCURACY, color="Bagging"), data=nb_bag_capacity_to_generalize) + geom_density(aes(x=ACCURACY, color="LOOCV"), data=nb_loocv_capacity_to_generalize) + geom_vline(xintercept = 1/3) + ggtitle("Naive Bayes Capacity to Generalize") + scale_color_brewer(palette = "Set1")
```

```{r}
ggplot() + geom_density(aes(x=ACCURACY), data=knn3_bag_capacity_to_generalize) + geom_vline(xintercept = 1/2)
```

```{r}
ggplot() + geom_density(aes(x=ACCURACY), data=knn5_bag_capacity_to_generalize) + geom_vline(xintercept = 2/3)
```